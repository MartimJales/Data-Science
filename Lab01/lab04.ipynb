{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab04 - MLP + RF + GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dslabs_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7072, 87)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv, DataFrame\n",
    "\n",
    "filename = \"data/CFD_smote.csv\"\n",
    "target = \"CLASS\"\n",
    "file_tag = \"CFD\"\n",
    "file = \"CFD\"\n",
    "data: DataFrame = read_csv(filename, na_values=\"\")\n",
    "\n",
    "train_filename = \"data/CFD_train.csv\"\n",
    "test_filename = \"data/CFD_test.csv\"\n",
    "\n",
    "eval_metric = \"accuracy\"\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels=[0, 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import array, ndarray\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.pyplot import figure, show\n",
    "from dslabs_functions import plot_multibar_chart\n",
    "from pandas import concat\n",
    "\n",
    "# Identificar as labels (classes) do target\n",
    "labels: list = list(data[target].unique())\n",
    "labels.sort()\n",
    "print(f\"Labels={labels}\")\n",
    "\n",
    "positive: int = 1\n",
    "negative: int = 0\n",
    "\n",
    "# Verificar a distribuição inicial\n",
    "values: dict[str, list[int]] = {\n",
    "    \"Original\": [\n",
    "        len(data[data[target] == negative]),\n",
    "        len(data[data[target] == positive]),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Separar X (features) e y (target)\n",
    "y: array = data.pop(target).to_list()  # Remove o target do dataset\n",
    "X: ndarray = data.values  # Obtém os valores das features como um ndarray\n",
    "\n",
    "# Dividir os dados em treino e teste (antes de aplicar MinMaxScaler)\n",
    "trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y, random_state=42)\n",
    "\n",
    "# Aplicar MinMaxScaler (apenas aos dados de treino e teste separadamente)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "trnX_scaled = scaler.fit_transform(trnX)  # Ajustar e transformar os dados de treino\n",
    "tstX_scaled = scaler.transform(tstX)      # Apenas transformar os dados de teste\n",
    "\n",
    "# Reconstruir os DataFrames de treino e teste, incluindo o target\n",
    "train: DataFrame = concat(\n",
    "    [DataFrame(trnX_scaled, columns=data.columns), DataFrame(trnY, columns=[target])],\n",
    "    axis=1,\n",
    ")\n",
    "train.to_csv(f\"data/{file_tag}_train.csv\", index=False)\n",
    "\n",
    "test: DataFrame = concat(\n",
    "    [DataFrame(tstX_scaled, columns=data.columns), DataFrame(tstY, columns=[target])],\n",
    "    axis=1,\n",
    ")\n",
    "test.to_csv(f\"data/{file_tag}_test.csv\", index=False)\n",
    "\n",
    "# Atualizar a distribuição de classes nos conjuntos de treino e teste\n",
    "values[\"Train\"] = [\n",
    "    len(train[train[target] == negative]),\n",
    "    len(train[train[target] == positive]),\n",
    "]\n",
    "values[\"Test\"] = [\n",
    "    len(test[test[target] == negative]),\n",
    "    len(test[test[target] == positive]),\n",
    "]\n",
    "\n",
    "# Plotar a distribuição de classes\n",
    "figure(figsize=(6, 4))\n",
    "plot_multibar_chart(labels, values, title=\"Data distribution per dataset\")\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_study_model(model, study_function):\n",
    "    trnX, tstX, trnY, tstY, labels, vars = read_train_test_from_files(\n",
    "        train_filename, test_filename, target\n",
    "    )\n",
    "\n",
    "    if 'Financial Distress' in vars:\n",
    "        col_idx = vars.index('Financial Distress')\n",
    "        trnX = np.delete(trnX, col_idx, axis=1)\n",
    "        tstX = np.delete(tstX, col_idx, axis=1)\n",
    "        vars.pop(col_idx)\n",
    "\n",
    "    print(f\"Train#={len(trnX)} Test#={len(tstX)}\")\n",
    "    print(f\"Labels={labels}\")\n",
    "\n",
    "    figure()\n",
    "    best_model, params = study_function(\n",
    "        trnX,\n",
    "        trnY,\n",
    "        tstX,\n",
    "        tstY,\n",
    "        nr_max_iterations=5000,\n",
    "        lag=500,\n",
    "        metric=eval_metric,\n",
    "        model=model,\n",
    "    )\n",
    "    savefig(f\"images/{file_tag}_{model}_{eval_metric}_study.png\")\n",
    "    show()\n",
    "\n",
    "    return best_model, params\n",
    "\n",
    "def evaluate_and_plot_model(best_model, params):\n",
    "    prd_trn = best_model.predict(trnX)\n",
    "    prd_tst = best_model.predict(tstX)\n",
    "\n",
    "    figure()\n",
    "    plot_evaluation_results(params, trnY, prd_trn, tstY, prd_tst, labels)\n",
    "    savefig(f'images/{file_tag}_{params[\"model\"]}_{params[\"name\"]}_best_{params[\"metric\"]}_eval.png')\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import array, ndarray\n",
    "# from pandas import read_csv, DataFrame\n",
    "\n",
    "# labels: list = list(data[target].unique())\n",
    "# labels.sort()\n",
    "# print(f\"Labels={labels}\")\n",
    "\n",
    "# positive: int = 1\n",
    "# negative: int = 0\n",
    "# values: dict[str, list[int]] = {\n",
    "#     \"Original\": [\n",
    "#         len(data[data[target] == negative]),\n",
    "#         len(data[data[target] == positive]),\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# y: array = data.pop(target).to_list()\n",
    "# X: ndarray = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import concat\n",
    "# from matplotlib.pyplot import figure, show\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from dslabs_functions import plot_multibar_chart\n",
    "\n",
    "\n",
    "# trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "\n",
    "# train: DataFrame = concat(\n",
    "#     [DataFrame(trnX, columns=data.columns), DataFrame(trnY, columns=[target])], axis=1\n",
    "# )\n",
    "# train.to_csv(f\"data/{file_tag}_train.csv\", index=False)\n",
    "\n",
    "# test: DataFrame = concat(\n",
    "#     [DataFrame(tstX, columns=data.columns), DataFrame(tstY, columns=[target])], axis=1\n",
    "# )\n",
    "# test.to_csv(f\"data/{file_tag}_test.csv\", index=False)\n",
    "\n",
    "# values[\"Train\"] = [\n",
    "#     len(train[train[target] == negative]),\n",
    "#     len(train[train[target] == positive]),\n",
    "# ]\n",
    "# values[\"Test\"] = [\n",
    "#     len(test[test[target] == negative]),\n",
    "#     len(test[test[target] == positive]),\n",
    "# ]\n",
    "\n",
    "# figure(figsize=(6, 4))\n",
    "# plot_multibar_chart(labels, values, title=\"Data distribution per dataset\")\n",
    "# show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: 'Financial Distress'\n",
      "Train#=4950 Test#=2122\n",
      "Labels=[0, 1]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from numpy import array, ndarray\n",
    "from matplotlib.pyplot import subplots, figure, savefig, show\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from dslabs_functions import (\n",
    "    CLASS_EVAL_METRICS,\n",
    "    DELTA_IMPROVE,\n",
    "    read_train_test_from_files,\n",
    ")\n",
    "from dslabs_functions import HEIGHT, plot_evaluation_results, plot_multiline_chart\n",
    "\n",
    "LAG: int = 500\n",
    "NR_MAX_ITER: int = 5000\n",
    "\n",
    "def mlp_study(\n",
    "    trnX: ndarray,\n",
    "    trnY: array,\n",
    "    tstX: ndarray,\n",
    "    tstY: array,\n",
    "    nr_max_iterations: int = 2500,\n",
    "    lag: int = 500,\n",
    "    metric: str = \"accuracy\",\n",
    ") -> tuple[MLPClassifier | None, dict]:\n",
    "    nr_iterations: list[int] = [lag] + [\n",
    "        i for i in range(2 * lag, nr_max_iterations + 1, lag)\n",
    "    ]\n",
    "\n",
    "    lr_types: list[Literal[\"constant\", \"invscaling\", \"adaptive\"]] = [\n",
    "        \"constant\",\n",
    "        \"invscaling\",\n",
    "        \"adaptive\",\n",
    "    ]  # only used if optimizer='sgd'\n",
    "    learning_rates: list[float] = [0.5, 0.05, 0.005, 0.0005]\n",
    "\n",
    "    best_model: MLPClassifier | None = None\n",
    "    best_params: dict = {\"name\": \"MLP\", \"metric\": metric, \"params\": ()}\n",
    "    best_performance: float = 0.0\n",
    "\n",
    "    values: dict = {}\n",
    "    _, axs = subplots(\n",
    "        1, len(lr_types), figsize=(len(lr_types) * HEIGHT, HEIGHT), squeeze=False\n",
    "    )\n",
    "    for i in range(len(lr_types)):\n",
    "        type: str = lr_types[i]\n",
    "        values = {}\n",
    "        for lr in learning_rates:\n",
    "            warm_start: bool = False\n",
    "            y_tst_values: list[float] = []\n",
    "            for j in range(len(nr_iterations)):\n",
    "                clf = MLPClassifier(\n",
    "                    learning_rate=type,\n",
    "                    learning_rate_init=lr,\n",
    "                    max_iter=lag,\n",
    "                    warm_start=warm_start,\n",
    "                    activation=\"logistic\",\n",
    "                    solver=\"sgd\",\n",
    "                    verbose=False,\n",
    "                )\n",
    "                clf.fit(trnX, trnY)\n",
    "                prdY: array = clf.predict(tstX)\n",
    "                eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "                y_tst_values.append(eval)\n",
    "                warm_start = True\n",
    "                if eval - best_performance > DELTA_IMPROVE:\n",
    "                    best_performance = eval\n",
    "                    best_params[\"params\"] = (type, lr, nr_iterations[j])\n",
    "                    best_model = clf\n",
    "                # print(f'MLP lr_type={type} lr={lr} n={nr_iterations[j]}')\n",
    "            values[lr] = y_tst_values\n",
    "        plot_multiline_chart(\n",
    "            nr_iterations,\n",
    "            values,\n",
    "            ax=axs[0, i],\n",
    "            title=f\"MLP with {type}\",\n",
    "            xlabel=\"nr iterations\",\n",
    "            ylabel=metric,\n",
    "            percentage=True,\n",
    "        )\n",
    "    print(\n",
    "        f'MLP best for {best_params[\"params\"][2]} iterations (lr_type={best_params[\"params\"][0]} and lr={best_params[\"params\"][1]}'\n",
    "    )\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "trnX, tstX, trnY, tstY, labels, vars = read_train_test_from_files(\n",
    "    train_filename, test_filename, target\n",
    ")\n",
    "\n",
    "# Drop the column 'Financial Distress' from both train and test feature sets\n",
    "if 'Financial Distress' in vars:\n",
    "    col_idx = vars.index('Financial Distress')  # Get the column index\n",
    "    trnX = np.delete(trnX, col_idx, axis=1)     # Drop from train set\n",
    "    tstX = np.delete(tstX, col_idx, axis=1)     # Drop from test set\n",
    "    vars.pop(col_idx)                           # Remove column name from the list\n",
    "    print(\"Dropped column: 'Financial Distress'\")\n",
    "\n",
    "print(f\"Train#={len(trnX)} Test#={len(tstX)}\")\n",
    "print(f\"Labels={labels}\")\n",
    "\n",
    "figure()\n",
    "best_model, params = mlp_study(\n",
    "    trnX,\n",
    "    trnY,\n",
    "    tstX,\n",
    "    tstY,\n",
    "    nr_max_iterations=NR_MAX_ITER,\n",
    "    lag=LAG,\n",
    "    metric=eval_metric,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_mlp_{eval_metric}_study.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_trn: array = best_model.predict(trnX)\n",
    "prd_tst: array = best_model.predict(tstX)\n",
    "figure()\n",
    "plot_evaluation_results(params, trnY, prd_trn, tstY, prd_tst, labels)\n",
    "savefig(f'images/{file_tag}_mlp_{params[\"name\"]}_best_{params[\"metric\"]}_eval.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_type: Literal[\"constant\", \"invscaling\", \"adaptive\"] = params[\"params\"][0]\n",
    "lr: float = params[\"params\"][1]\n",
    "\n",
    "nr_iterations: list[int] = [LAG] + [i for i in range(2 * LAG, NR_MAX_ITER + 1, LAG)]\n",
    "\n",
    "y_tst_values: list[float] = []\n",
    "y_trn_values: list[float] = []\n",
    "acc_metric = \"accuracy\"\n",
    "\n",
    "warm_start: bool = False\n",
    "for n in nr_iterations:\n",
    "    clf = MLPClassifier(\n",
    "        warm_start=warm_start,\n",
    "        learning_rate=lr_type,\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=n,\n",
    "        activation=\"logistic\",\n",
    "        solver=\"sgd\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_tst_Y: array = clf.predict(tstX)\n",
    "    prd_trn_Y: array = clf.predict(trnX)\n",
    "    y_tst_values.append(CLASS_EVAL_METRICS[acc_metric](tstY, prd_tst_Y))\n",
    "    y_trn_values.append(CLASS_EVAL_METRICS[acc_metric](trnY, prd_trn_Y))\n",
    "    warm_start = True\n",
    "\n",
    "figure()\n",
    "plot_multiline_chart(\n",
    "    nr_iterations,\n",
    "    {\"Train\": y_trn_values, \"Test\": y_tst_values},\n",
    "    title=f\"MLP overfitting study for lr_type={lr_type} and lr={lr}\",\n",
    "    xlabel=\"nr_iterations\",\n",
    "    ylabel=str(eval_metric),\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_mlp_{eval_metric}_overfitting.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arange\n",
    "from dslabs_functions import plot_line_chart\n",
    "\n",
    "\n",
    "figure()\n",
    "plot_line_chart(\n",
    "    arange(len(best_model.loss_curve_)),\n",
    "    best_model.loss_curve_,\n",
    "    title=\"Loss curve for MLP best model training\",\n",
    "    xlabel=\"iterations\",\n",
    "    ylabel=\"loss\",\n",
    "    percentage=False,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_mlp_{eval_metric}_loss_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, ndarray\n",
    "from matplotlib.pyplot import figure, savefig, show\n",
    "from sklearn.svm import SVC\n",
    "from dslabs_functions import (\n",
    "    CLASS_EVAL_METRICS,\n",
    "    DELTA_IMPROVE,\n",
    "    read_train_test_from_files,\n",
    ")\n",
    "from dslabs_functions import plot_evaluation_results, plot_multiline_chart\n",
    "\n",
    "\n",
    "def svm_study(\n",
    "    trnX: ndarray,\n",
    "    trnY: array,\n",
    "    tstX: ndarray,\n",
    "    tstY: array,\n",
    "    nr_max_iterations: int = 2500,\n",
    "    lag: int = 500,\n",
    "    metric: str = \"accuracy\",\n",
    ") -> tuple[SVC | None, dict]:\n",
    "    nr_iterations: list[int] = [100] + [\n",
    "        i for i in range(500, nr_max_iterations + 1, lag)\n",
    "    ]\n",
    "\n",
    "    kernel_types: list[str] = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "    poly_degrees: list[int] = [2, 3, 4]\n",
    "\n",
    "    best_model = None\n",
    "    best_params: dict = {\"name\": \"SVM\", \"metric\": metric, \"params\": ()}\n",
    "    best_performance: float = 0.0\n",
    "\n",
    "    values: dict = {}\n",
    "    for kernel in kernel_types:\n",
    "        degrees: list[int] = poly_degrees if \"poly\" == kernel else [0]\n",
    "        for d in degrees:\n",
    "            kernel_name: str = f\"poly_{d}\" if \"poly\" == kernel else kernel\n",
    "            y_tst_values: list[float] = []\n",
    "            for n in nr_iterations:\n",
    "                clf = SVC(kernel=kernel, max_iter=n, degree=d, verbose=False)\n",
    "                clf.fit(trnX, trnY)\n",
    "                prdY: array = clf.predict(tstX)\n",
    "                eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "                y_tst_values.append(eval)\n",
    "                if eval - best_performance > DELTA_IMPROVE:\n",
    "                    best_performance = eval\n",
    "                    best_params[\"params\"] = (kernel, n, d)\n",
    "                    best_model = clf\n",
    "                # print(f'SVM lr_type={kernel_name} n={n} -> {eval}')\n",
    "            values[kernel_name] = y_tst_values\n",
    "    plot_multiline_chart(\n",
    "        nr_iterations,\n",
    "        values,\n",
    "        title=f\"SVM models ({metric})\",\n",
    "        xlabel=\"nr iterations\",\n",
    "        ylabel=metric,\n",
    "        percentage=True,\n",
    "    )\n",
    "    best_kernel = best_params[\"params\"][0]\n",
    "    kernel_name = (\n",
    "        f'poly_{best_params[\"params\"][2]}' if \"poly\" == best_kernel else best_kernel\n",
    "    )\n",
    "    print(f'SVM best for {kernel_name} and n={best_params[\"params\"][1]}')\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "trnX, tstX, trnY, tstY, labels, vars = read_train_test_from_files(\n",
    "    train_filename, test_filename, target\n",
    ")\n",
    "\n",
    "# Drop the column 'Financial Distress' from both train and test feature sets\n",
    "if 'Financial Distress' in vars:\n",
    "    col_idx = vars.index('Financial Distress')  # Get the column index\n",
    "    trnX = np.delete(trnX, col_idx, axis=1)     # Drop from train set\n",
    "    tstX = np.delete(tstX, col_idx, axis=1)     # Drop from test set\n",
    "    vars.pop(col_idx)                           # Remove column name from the list\n",
    "\n",
    "print(f\"Train#={len(trnX)} Test#={len(tstX)}\")\n",
    "print(f\"Labels={labels}\")\n",
    "\n",
    "figure()\n",
    "best_model, params = svm_study(\n",
    "    trnX,\n",
    "    trnY,\n",
    "    tstX,\n",
    "    tstY,\n",
    "    nr_max_iterations=5000,\n",
    "    lag=500,\n",
    "    metric=eval_metric,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_svm_{eval_metric}_study.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_trn: array = best_model.predict(trnX)\n",
    "prd_tst: array = best_model.predict(tstX)\n",
    "figure()\n",
    "plot_evaluation_results(params, trnY, prd_trn, tstY, prd_tst, labels)\n",
    "savefig(f'images/{file_tag}_svm_{params[\"name\"]}_best_{params[\"metric\"]}_eval.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables relevancy - I am not sure if this is the right stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from matplotlib.pyplot import imread, imshow, axis\n",
    "from subprocess import call\n",
    "\n",
    "tree_filename: str = f\"images/{file_tag}_rf_{eval_metric}_best_tree\"\n",
    "max_depth2show = 3\n",
    "st_labels: list[str] = [str(value) for value in labels]\n",
    "\n",
    "dot_data: str = export_graphviz(\n",
    "    best_model,\n",
    "    out_file=tree_filename + \".dot\",\n",
    "    max_depth=max_depth2show,\n",
    "    feature_names=vars,\n",
    "    class_names=st_labels,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    impurity=False,\n",
    "    special_characters=True,\n",
    "    precision=2,\n",
    ")\n",
    "# Convert to png\n",
    "call(\n",
    "    [\"dot\", \"-Tpng\", tree_filename + \".dot\", \"-o\", tree_filename + \".png\", \"-Gdpi=600\"]\n",
    ")\n",
    "\n",
    "figure(figsize=(14, 6))\n",
    "imshow(imread(tree_filename + \".png\"))\n",
    "axis(\"off\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel: str = params[\"params\"][0]\n",
    "degree: int = params[\"params\"][2]\n",
    "kernel_name: str = f'poly (d={params[\"params\"][2]})' if \"poly\" == kernel else kernel\n",
    "nr_iterations: list[int] = [100] + [i for i in range(500, 5001, 500)]\n",
    "\n",
    "y_tst_values: list[float] = []\n",
    "y_trn_values: list[float] = []\n",
    "acc_metric: str = \"accuracy\"\n",
    "\n",
    "warm_start: bool = False\n",
    "for n in nr_iterations:\n",
    "    clf = SVC(kernel=kernel, max_iter=n, degree=degree, verbose=False)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_tst_Y: array = clf.predict(tstX)\n",
    "    prd_trn_Y: array = clf.predict(trnX)\n",
    "    y_tst_values.append(CLASS_EVAL_METRICS[acc_metric](tstY, prd_tst_Y))\n",
    "    y_trn_values.append(CLASS_EVAL_METRICS[acc_metric](trnY, prd_trn_Y))\n",
    "    warm_start = True\n",
    "\n",
    "figure()\n",
    "plot_multiline_chart(\n",
    "    nr_iterations,\n",
    "    {\"Train\": y_trn_values, \"Test\": y_tst_values},\n",
    "    title=f\"SVM overfitting study for {kernel_name}\",\n",
    "    xlabel=\"nr_iterations\",\n",
    "    ylabel=str(eval_metric),\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_svm_{eval_metric}_overfitting.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, ndarray\n",
    "from matplotlib.pyplot import subplots, figure, savefig, show\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dslabs_functions import (\n",
    "    CLASS_EVAL_METRICS,\n",
    "    DELTA_IMPROVE,\n",
    "    read_train_test_from_files,\n",
    ")\n",
    "from dslabs_functions import HEIGHT, plot_evaluation_results, plot_multiline_chart\n",
    "\n",
    "\n",
    "def random_forests_study(\n",
    "    trnX: ndarray,\n",
    "    trnY: array,\n",
    "    tstX: ndarray,\n",
    "    tstY: array,\n",
    "    nr_max_trees: int = 2500,\n",
    "    lag: int = 500,\n",
    "    metric: str = \"accuracy\",\n",
    ") -> tuple[RandomForestClassifier | None, dict]:\n",
    "    n_estimators: list[int] = [100] + [i for i in range(500, nr_max_trees + 1, lag)]\n",
    "    max_depths: list[int] = [2, 5, 7]\n",
    "    max_features: list[float] = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "    best_model: RandomForestClassifier | None = None\n",
    "    best_params: dict = {\"name\": \"RF\", \"metric\": metric, \"params\": ()}\n",
    "    best_performance: float = 0.0\n",
    "\n",
    "    values: dict = {}\n",
    "\n",
    "    cols: int = len(max_depths)\n",
    "    _, axs = subplots(1, cols, figsize=(cols * HEIGHT, HEIGHT), squeeze=False)\n",
    "    for i in range(len(max_depths)):\n",
    "        d: int = max_depths[i]\n",
    "        values = {}\n",
    "        for f in max_features:\n",
    "            y_tst_values: list[float] = []\n",
    "            for n in n_estimators:\n",
    "                clf = RandomForestClassifier(\n",
    "                    n_estimators=n, max_depth=d, max_features=f\n",
    "                )\n",
    "                clf.fit(trnX, trnY)\n",
    "                prdY: array = clf.predict(tstX)\n",
    "                eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "                y_tst_values.append(eval)\n",
    "                if eval - best_performance > DELTA_IMPROVE:\n",
    "                    best_performance = eval\n",
    "                    best_params[\"params\"] = (d, f, n)\n",
    "                    best_model = clf\n",
    "                # print(f'RF d={d} f={f} n={n}')\n",
    "            values[f] = y_tst_values\n",
    "        plot_multiline_chart(\n",
    "            n_estimators,\n",
    "            values,\n",
    "            ax=axs[0, i],\n",
    "            title=f\"Random Forests with max_depth={d}\",\n",
    "            xlabel=\"nr estimators\",\n",
    "            ylabel=metric,\n",
    "            percentage=True,\n",
    "        )\n",
    "    print(\n",
    "        f'RF best for {best_params[\"params\"][2]} trees (d={best_params[\"params\"][0]} and f={best_params[\"params\"][1]})'\n",
    "    )\n",
    "    return best_model, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model, params = process_and_study_model(\"rf\", random_forests_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate_and_plot_model(best_model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import std, argsort\n",
    "from dslabs_functions import plot_horizontal_bar_chart\n",
    "\n",
    "stdevs: list[float] = list(\n",
    "    std([tree.feature_importances_ for tree in best_model.estimators_], axis=0)\n",
    ")\n",
    "importances = best_model.feature_importances_\n",
    "indices: list[int] = argsort(importances)[::-1]\n",
    "elems: list[str] = []\n",
    "imp_values: list[float] = []\n",
    "for f in range(len(vars)):\n",
    "    elems += [vars[indices[f]]]\n",
    "    imp_values.append(importances[indices[f]])\n",
    "    print(f\"{f+1}. {elems[f]} ({importances[indices[f]]})\")\n",
    "\n",
    "figure()\n",
    "plot_horizontal_bar_chart(\n",
    "    elems,\n",
    "    imp_values,\n",
    "    error=stdevs,\n",
    "    title=\"RF variables importance\",\n",
    "    xlabel=\"importance\",\n",
    "    ylabel=\"variables\",\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_rf_{eval_metric}_vars_ranking.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_max: int = params[\"params\"][0]\n",
    "feat: float = params[\"params\"][1]\n",
    "nr_estimators: list[int] = [i for i in range(2, 2501, 500)]\n",
    "\n",
    "y_tst_values: list[float] = []\n",
    "y_trn_values: list[float] = []\n",
    "acc_metric: str = \"accuracy\"\n",
    "\n",
    "for n in nr_estimators:\n",
    "    clf = RandomForestClassifier(n_estimators=n, max_depth=d_max, max_features=feat)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_tst_Y: array = clf.predict(tstX)\n",
    "    prd_trn_Y: array = clf.predict(trnX)\n",
    "    y_tst_values.append(CLASS_EVAL_METRICS[acc_metric](tstY, prd_tst_Y))\n",
    "    y_trn_values.append(CLASS_EVAL_METRICS[acc_metric](trnY, prd_trn_Y))\n",
    "\n",
    "figure()\n",
    "plot_multiline_chart(\n",
    "    nr_estimators,\n",
    "    {\"Train\": y_trn_values, \"Test\": y_tst_values},\n",
    "    title=f\"RF overfitting study for d={d_max} and f={feat}\",\n",
    "    xlabel=\"nr_estimators\",\n",
    "    ylabel=str(eval_metric),\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_rf_{eval_metric}_overfitting.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, ndarray\n",
    "from matplotlib.pyplot import subplots, figure, savefig, show\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from dslabs_functions import (\n",
    "    CLASS_EVAL_METRICS,\n",
    "    DELTA_IMPROVE,\n",
    "    read_train_test_from_files,\n",
    ")\n",
    "from dslabs_functions import HEIGHT, plot_evaluation_results, plot_multiline_chart\n",
    "\n",
    "\n",
    "def gradient_boosting_study(\n",
    "    trnX: ndarray,\n",
    "    trnY: array,\n",
    "    tstX: ndarray,\n",
    "    tstY: array,\n",
    "    nr_max_trees: int = 2500,\n",
    "    lag: int = 500,\n",
    "    metric: str = \"accuracy\",\n",
    ") -> tuple[GradientBoostingClassifier | None, dict]:\n",
    "    n_estimators: list[int] = [100] + [i for i in range(500, nr_max_trees + 1, lag)]\n",
    "    max_depths: list[int] = [2, 5, 7]\n",
    "    learning_rates: list[float] = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "    best_model: GradientBoostingClassifier | None = None\n",
    "    best_params: dict = {\"name\": \"GB\", \"metric\": metric, \"params\": ()}\n",
    "    best_performance: float = 0.0\n",
    "\n",
    "    values: dict = {}\n",
    "    cols: int = len(max_depths)\n",
    "    _, axs = subplots(1, cols, figsize=(cols * HEIGHT, HEIGHT), squeeze=False)\n",
    "    for i in range(len(max_depths)):\n",
    "        d: int = max_depths[i]\n",
    "        values = {}\n",
    "        for lr in learning_rates:\n",
    "            y_tst_values: list[float] = []\n",
    "            for n in n_estimators:\n",
    "                clf = GradientBoostingClassifier(\n",
    "                    n_estimators=n, max_depth=d, learning_rate=lr\n",
    "                )\n",
    "                clf.fit(trnX, trnY)\n",
    "                prdY: array = clf.predict(tstX)\n",
    "                eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "                y_tst_values.append(eval)\n",
    "                if eval - best_performance > DELTA_IMPROVE:\n",
    "                    best_performance = eval\n",
    "                    best_params[\"params\"] = (d, lr, n)\n",
    "                    best_model = clf\n",
    "                # print(f'GB d={d} lr={lr} n={n}')\n",
    "            values[lr] = y_tst_values\n",
    "        plot_multiline_chart(\n",
    "            n_estimators,\n",
    "            values,\n",
    "            ax=axs[0, i],\n",
    "            title=f\"Gradient Boosting with max_depth={d}\",\n",
    "            xlabel=\"nr estimators\",\n",
    "            ylabel=metric,\n",
    "            percentage=True,\n",
    "        )\n",
    "    print(\n",
    "        f'GB best for {best_params[\"params\"][2]} trees (d={best_params[\"params\"][0]} and lr={best_params[\"params\"][1]}'\n",
    "    )\n",
    "\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, params = process_and_study_model(\"gb\", gradient_boosting_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_plot_model(best_model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import std, argsort\n",
    "from dslabs_functions import plot_horizontal_bar_chart\n",
    "\n",
    "trees_importances: list[float] = []\n",
    "for lst_trees in best_model.estimators_:\n",
    "    for tree in lst_trees:\n",
    "        trees_importances.append(tree.feature_importances_)\n",
    "\n",
    "stdevs: list[float] = list(std(trees_importances, axis=0))\n",
    "importances = best_model.feature_importances_\n",
    "indices: list[int] = argsort(importances)[::-1]\n",
    "elems: list[str] = []\n",
    "imp_values: list[float] = []\n",
    "for f in range(len(vars)):\n",
    "    elems += [vars[indices[f]]]\n",
    "    imp_values.append(importances[indices[f]])\n",
    "    print(f\"{f+1}. {elems[f]} ({importances[indices[f]]})\")\n",
    "\n",
    "figure()\n",
    "plot_horizontal_bar_chart(\n",
    "    elems,\n",
    "    imp_values,\n",
    "    error=stdevs,\n",
    "    title=\"GB variables importance\",\n",
    "    xlabel=\"importance\",\n",
    "    ylabel=\"variables\",\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_gb_{eval_metric}_vars_ranking.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_max: int = params[\"params\"][0]\n",
    "lr: float = params[\"params\"][1]\n",
    "nr_estimators: list[int] = [i for i in range(2, 2501, 500)]\n",
    "\n",
    "y_tst_values: list[float] = []\n",
    "y_trn_values: list[float] = []\n",
    "acc_metric: str = \"accuracy\"\n",
    "\n",
    "for n in nr_estimators:\n",
    "    clf = GradientBoostingClassifier(n_estimators=n, max_depth=d_max, learning_rate=lr)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_tst_Y: array = clf.predict(tstX)\n",
    "    prd_trn_Y: array = clf.predict(trnX)\n",
    "    y_tst_values.append(CLASS_EVAL_METRICS[acc_metric](tstY, prd_tst_Y))\n",
    "    y_trn_values.append(CLASS_EVAL_METRICS[acc_metric](trnY, prd_trn_Y))\n",
    "\n",
    "figure()\n",
    "plot_multiline_chart(\n",
    "    nr_estimators,\n",
    "    {\"Train\": y_trn_values, \"Test\": y_tst_values},\n",
    "    title=f\"GB overfitting study for d={d_max} and lr={lr}\",\n",
    "    xlabel=\"nr_estimators\",\n",
    "    ylabel=str(eval_metric),\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_gb_{eval_metric}_overfitting.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
